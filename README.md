# TP-Scrapping

Permet de scrap le blog du mod√©rateur. (N'est pas reli√© √† une base mongodb, enregistre les √©l√©ments dans un json : data\articles.json )

## üöÄ Installation

```bash
# Cloner le projet
git clone <url-du-repo>
cd TP-Scrapping

# Installer les d√©pendances
pip install -r requirements.txt
```

## üìã Fonctionnalit√©s

- Scraping par cat√©gorie (web, social-media, mobile, digital, e-commerce, tech)
- Extraction compl√®te du contenu des articles (titre, auteur, date, contenu, images)
- Sauvegarde en JSON et export CSV
- Gestion des d√©lais entre requ√™tes
- Statistiques d√©taill√©es

## üõ†Ô∏è Utilisation

### Commandes principales

#### 1. Scraper une cat√©gorie sp√©cifique
```bash
python main.py scrape web 2
```
**R√©sultat :**
```
D√©marrage du scraping de la cat√©gorie 'web'
URL: https://www.blogdumoderateur.com/web/
Pages √† scraper: 2
==================================================
r√©cup√©ration de la liste des articles...
Trouv√© 15 articles √† scraper
==================================================
Scraping article 1/15: Les meilleures extensions Chrome...
  ‚úì Article scraped avec succ√®s
Scraping article 2/15: React 18 : les nouveaut√©s...
  ‚úì Article scraped avec succ√®s
...
Scraping termin√©. 15/15 articles r√©cup√©r√©s
Articles sauvegard√©s. Total dans la base: 15
```

#### 2. Scraper toutes les cat√©gories
```bash
python main.py scrape-all 1
```
**R√©sultat :**
```
D√©marrage du scraping de toutes les cat√©gories
Cat√©gories: ['web', 'social-media', 'mobile', 'digital', 'e-commerce', 'tech']
Pages par cat√©gorie: 1
============================================================

D√©marrage de la cat√©gorie: web
Trouv√© 8 articles √† scraper
Scraping termin√©. 8/8 articles r√©cup√©r√©s

D√©marrage de la cat√©gorie: social-media
Trouv√© 7 articles √† scraper
...
============================================================
Scraping global termin√©. Total d'articles r√©cup√©r√©s: 42
```

#### 3. Afficher les statistiques
```bash
python main.py stats
```
**R√©sultat :**
```
STATISTIQUES DE LA BASE DE DONN√âES
========================================
Total d'articles: 42
Nombre de cat√©gories: 6
Nombre d'auteurs: 12
Articles avec images: 38
Articles avec contenu: 42

R√©partition par cat√©gorie:
  - web: 8 articles
  - social-media: 7 articles
  - mobile: 6 articles
  - digital: 8 articles
  - e-commerce: 7 articles
  - tech: 6 articles
```

#### 4. Exporter vers CSV
```bash
python main.py export
```
**R√©sultat :**
```
Export CSV cr√©√© : data/articles_export.csv
42 articles export√©s avec succ√®s
```

### Options disponibles

| Commande | Description | Exemple |
|----------|-------------|---------|
| `scrape <cat√©gorie> [pages]` | Scrape une cat√©gorie | `python main.py scrape web 3` |
| `scrape-all [pages]` | Scrape toutes les cat√©gories | `python main.py scrape-all 2` |
| `stats` | Affiche les statistiques | `python main.py stats` |
| `export` | Exporte vers CSV | `python main.py export` |

### Cat√©gories disponibles
- `web` - D√©veloppement web
- `social-media` - R√©seaux sociaux  
- `mobile` - Applications mobiles
- `digital` - Transformation digitale
- `e-commerce` - Commerce √©lectronique
- `tech` - Technologies

## üìÅ Structure des donn√©es

### Fichiers g√©n√©r√©s
- `data/articles.json` - Base de donn√©es compl√®te
- `data/articles_export.csv` - Export pour analyse

### Format des donn√©es
Chaque article contient :
```json
{
  "title": "Titre de l'article",
  "author": "Nom de l'auteur", 
  "date": "2025-01-15",
  "url": "https://...",
  "content": "Contenu complet...",
  "images": ["url1.jpg", "url2.jpg"],
  "category_scraped": "web"
}
```

## ‚öôÔ∏è Configuration

Le d√©lai entre requ√™tes est fix√© √† 1 seconde par d√©faut pour respecter le serveur.

## üîß Structure du projet

```
TP-Scrapping/
‚îú‚îÄ‚îÄ src/                    # Modules de scraping
‚îú‚îÄ‚îÄ utils/                  # Utilitaires
‚îú‚îÄ‚îÄ data/                   # Donn√©es sauvegard√©es  
‚îú‚îÄ‚îÄ main.py                 # Point d'entr√©e
‚îî‚îÄ‚îÄ requirements.txt        # D√©pendances
```

### TP r√©alis√© en mode Vibes Coding !